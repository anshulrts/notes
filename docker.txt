*Docker*

Quick Commands
Container
List all containers and their status: docker ps -a (To find all running processes, remove flag -a)
Run Container: docker run <image_id/name>
Stop Container: docker stop <container_id or name>
Remove after stop: docker run --rm <imageid/name>
Give name while running container: docker run --name <container name> <image id/name>
Run container in interactive mode: docker run -it <imageid/name>
Restart existing container: docker start <container_name/id>
Restart in attached Mode: docker start -a <container_name/id>
Run & Expose port: docker run -p 5888:3000 <image_id> (5888 is local port, 3000 is what image has exposed)
Remove/clean containers: docker rm <container_name/id> 
view logs from a container: docker log <containerid/name>
Copy files from host to a container: docker cp temp/test.txt <container_name>:path (e.g mynode-container/)
- If the folder doesn't exist, it'll create one automatically
Copy files from container to host: docker cp <containerid/name>:/temp/test.txt temp
Get details of container: docker container inspect <containerid/name>

docker run --rm --name myapp -it -p 5888:3000 myimage
The above command will remove a container when it stops, has name myapp, is running in interactive mode,
has a port open on docker host at 5888 and exposes port 3000 from container and is based on image myimage

Image
List all images: docker images
Build Image: docker build <path to dockerfile or . in case of current folder>
Name an image while building: docker build -t name:tag (eg node:latest)
Remove/clean images: docker rmi <image name/id> (Note - You cannot remove an image for which there's a referencing container)

Docker has 2 core concepts
- Images
- Containers

Images could be either custom images or pre built.
Containers run from Images i.e image is just a blueprint/template using which containers run.
Images are search for either on your local, but if they are not available, on docker hub(by default)/any other repo you specify.


*Using Pre-Build Images*
For eg, "node" is an image publically available on docker hub published by official Node team. 
In order to access it, you have to run this:
	docker run -it node
(The reason we need to have interactive flag here is because we want to run our commands on the shell)

*Custom Images*
To create custom images, in the root(not necessary) of your repo create a dockerfile and call it "Dockerfile"
This is a sample of how it looks like:
FROM node

WORKDIR /app

COPY . /app

RUN npm install

EXPOSE 3000

CMD ["node", "server.js"]

*Images are Read Only*
Let's say you build an image. After that you change your code, you'll have to re-build the image.

Also note that Docker is smart enough to understand whether there's any change to any of the instruction.
Docker use the cache for the steps in dockerfile which are not changed. But all the steps after the first changed step will be executed fresh.
Therefore, in the above dockerfile, it makes sense to first copy packages.json to /app, run npm install step and then copy other code.
This is because code change is going to happen frequently compared to adding new packges - you'll be running npm install everytime unnecessarily
Here's the updated dockerfile

FROM node

WORKDIR /app

COPY package.json /app

RUN npm install

COPY . /app

EXPOSE 3000

CMD ["node", "server.js"]

*Layer Based Architecture*
Every instruction in dockerfile is layer in the image. So, when we run a container over an image, it's basically just asking for another layer (which it gets by execuing the last CMD command)

By default, docker run command runs in attached mode, but docker start command is in detached mode.


*Storage in Docker*
Broadly divided into 2 types
- Volumes (named and unnamed) (Managed by docker)
- Bind Mounts (Managed by us)

*Volumes*
These are types of mounts which cannot be seen on a host directly, but are managed by docker.

Unnamed volume
As the name suggests, there are types of volumes which are not named. They are available only till the time container is active,
or when the same container is restarted. They are removed when a container is removed.
There are 2 ways we can create these, we usually prefer 2nd:
1) In dockerfile
in docker file, add this statement:
VOLUME [ "/app/feedback" ]
what this basically says is that whatever file is present inside of the container at /app/feedback/, save it in docker volume
2) through docker command
docker run --rm -v /app/feedback myimage

named volumes
They are given names and they survive container stops.
This is how we define named values
docker run --rm -v feedback:/app/feedback myimage
Now if you want to use the files in this, use the same -v option

*Bind Mounts*
In these, we set paths on docker host (host machine) which running the container.

docker run --rm -v "Users/temp/feedback":/app/feedback myimage

But remember nodu_modules was lost, because bind mount replace the files present in container.
You can use unnamed volumes in this case.

Usually we use Named volumes the most.

*Networks*
There are 3 types of communication that can happen with docker containers
1) Container to www (whole internet) - This is allowed by default
Eg you have a node app, which is calling API's hosted on https://swapi.dev/api/films
This communication is allowed by default.
2) Container to Local Host Machine Communication
Eg you have a node application and want it to connect to mongodb installed on your machine
To make this communication happen, set the url's domain to host.docker.internal
Eg to connect to mongodb, use this url - mongodb://host.docker.internal:27017/dbname
3) Container to Container Communication
Eg your node application container connects to mongodb container
For this, you need to create a network, using this command:
docker network create my-net
The run container with --network option like this-
docker run -d --name my-container-name --network my-net <imageid/name>
When you run 2 containers in same network, you can refer them using container name in the code.
Put container name in the domain of URL
When you create networks, you don't need to expose the port using -p flag.

.Net Apps
Till 2019, microsoft used to host dotnet images in docker hub, but now it does it on their own website (mcr.microsoft.com)
With .Net apps, we use stages in docker. This is because we need different images of dotnet to build vs run the apps.
Above example was for node, which is a scripting language and running the file directly works, but C# is compiled language, we need to run the compiled file.
Each FROM statement introduce a new stage in docker.
For .net6.0, this is how a usual docker file looks like:

FROM mcr.microsoft.com/dotnet/sdk:6.0 as build-env
WORKDIR /App
COPY . ./
RUN dotnet restore
RUN dotnet publish -c Release -o out

FROM mcr.microsoft.com/dotnet/runtime:6.0
WORKDIR /App
COPY --from=build-env /App/out .
CMD ["dotnet", "CSharpConcepts.dll"]

Each FROM line indicates a new stage. The only thing that survives is the final stage, where all that's being done is the published files are copied in and
the app is run, giving you an optimally sized image.
In this case, if you used a single staged build and built this image based on sdk, image size came out to be 800+MB.
But with multi-stage, and building the image on top of dotnet/runtime, we got only 208MB image.



DOCKER COMPOSE
It is used to spin up multiple containers.
This is a typical compose file:

    version: "3.8"

    services:
    authorization:
        image: abc.authorization
        container_name: abc.authorization
        build:
        context: .
        dockerfile: authorization.dockerfile
        ports: 
        - "9002:8080"
        - "9001:8081"
        environment:
        - ConnectionStrings__Database=server=host.docker.internal,1433;database=db_name;user id=sa;password=P@ssw0rd;MultipleActiveResultSets=true;trustservercertificate=true
        - ConnectionStrings__Database1=server=host.docker.internal,1433;database=db_name;user id=sa;password=P@ssw0rd;MultipleActiveResultSets=true;trustservercertificate=true
        networks:
        - token-exchange-poc

    destination:
        image: wso.destination
        container_name: wso.destination
        build:
        context: .
        dockerfile: destination.dockerfile
        ports: 
        - "9004:8080"
        - "9003:8081"
        networks:
        - token-exchange-poc

    gateway:
        image: wso.gateway
        container_name: wso.gateway
        build:
        context: .
        dockerfile: gateway.dockerfile
        ports:
        - "9006:8080"
        - "9005:8081"
        environment:
        - AuthorizationService=authorization:8080
        - RedisPath=redis:6379
        - GlobalConfiguration__BaseUrl=http://destination:8080
        networks:
        - token-exchange-poc
        depends_on:
        - redis

    identity-provider:
        image: wso.identity-provider
        container_name: wso.identity-provider
        build:
        context: .
        dockerfile: identityprovider.dockerfile
        ports:
        - "9008:8080"
        - "9007:8081"
        networks:
        - token-exchange-poc

    redis:
        image: redis
        ports:
        - 6379:6379
        networks:
        - token-exchange-poc

    networks:
    token-exchange-poc:
        external: true

Whatever environment variable you provide in -environment, it will override whatever is defined inside